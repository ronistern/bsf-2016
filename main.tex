\documentclass[12pt]{article}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{graphicx}
\usepackage{times}
\usepackage{anysize}
\usepackage{hyperref}
\marginsize{2cm}{2cm}{2cm}{3cm}
%\onehalfspace
%\doublespace
\setlength{\parindent}{0.8cm}
%\setlength{\parskip}{0.2\baselineskip}
%\setlength{\topmargin}{2cm}
%\setlength{\textheight}{25cm}
%\setlength{\textwidth}{14cm}
%\setlength{\oddsidemargin}{2cm}
%\setlength{\evensidemargin}{2cm}
\usepackage{fancyhdr}
\pagestyle{fancy}

\newcommand{\note}[1]{\textbf{\textit{#1}}}
\newcommand{\astar}{A$^*$}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\eff}{\textit{eff}}
\newcommand{\pre}{\textit{pre}}

\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{corollary}{Corollary}

\linespread{1.3}

\lhead{Data-and-Model Driven Reasoning} \rhead{R. Stern and B. Juba}
\cfoot{\thepage} 
%\cfoot{} 
\pagenumbering{arabic}
%\pagenumbering{Roman}

\begin{document}

%\title{Learning Common Sense Rules Multi-Agent Reasoning with Common}
%\title{Augmenting Model-based Approaches for Diagnosis, Planning, and Plan Recognition with Data}
%\title{Diagnosis, Planning, and Plan Recognition with An Approximate Model and an Abundance of Data}
\title{Data-and-Model Driven Reasoning}
%\note{Need to shorten to 250 words}

\begin{center}
\LARGE{Research Plan}
\end{center}

\section{Scientific and Technological Background}
% 1. From BSF guidelines: A brief description of the subject and the scientific and technological background;
\note{Roni: this section needs a makeover once the methodology section is completed -- removing the plan recognition and leaving better clues for the rest of the work}


% Reasoning is really important. Example of reasoniong for planning, diagnosis, and plan recognition
{\bf Reasoning} is ``the process of thinking about something in a logical way in order to form a conclusion or judgment''~\cite{reasoning2016dictionary}, and is commonly attributed as a task that requires some intelligence. Indeed, much of the Artificial Intelligence (AI) research has been devoted to developing methods for reasoning, and effective reasoning is at the heart of classical AI tasks such as planning, diagnosis, and plan recognition. When planning, one needs to reason about possible future actions it may take in order to achieve a desired goal. When diagnosing, one needs to reason about possible explanations to an abnormally behaving systems. Plan recognition can be viewed as a merge of diagnosis and planning: one needs to reason about possible plans an other agent is performing. Indeed, all of these tasks, which are at the core of many AI algorithms and systems, require some form of reasoning.  


%it is the task of identifying the plan an agent is following by observing its behavior. All tasks are commonly studied in the AI literature and are at the core of many AI algorithms and systems. 

%, diagnosis detection and isolation, and recognition. %algorithms that perform tasks such as planning, diagnosing, and others. %an active component in the development of many AI algorithms

%This proposal focuses on three main tasks: automated diagnosis, automated planning, and plan recognition. The first task (diagnosis) is to find identify faulty components in an abnormally behaving system. The second task (planning) is to create a plan for an agent to follow in order to achieve a designated goal. The third task (plan recognition) can be viewed as a merge of diagnosis and planning: it is the task of identifying the plan an agent is following by observing its behavior. All tasks are commonly studied in the AI literature and are at the core of many AI algorithms and systems. 



% The traditional AI approaches are model-based - they assume a model is given
Traditional AI approached these problems using {\em model-based reasoning}, where an underlying model of the world is assumed and is used to find effective solutions. For example, in classical STRIPS planning~\cite{fikes1971strips} the assumed model of the world is  represented by the initial state, actions' preconditions and effects, and the ``frame axioms''~\cite{ghallab2004automated}. 
%This model of the world is used by planning algorithms to search through the space of possible plans. % until finding one that achieves the desired goals. 
In model-based diagnosis (MBD) it assumed that a model of how the diagnosed system is expected to behave is given. In plan recognitions, In plan recognition, the corresponding assumption is that we know the agent's planning capabilities. Indeed, many plan recognition algorithms assume that this is given, e.g., in the form of a plan library that allows identifying all possible plans the agent may be following. % Maybe add here examples of PR algorithms? e.g,. PHATT

%is a prominent approach in the literature for automated diagnosis, which assumes a model of how the diagnosed system is expected to behave is given. Then, MBD algorithms use Truth Maintenance Systems (TMS)~\cite{deKleer1986assumption} or other logical methods to infer from this model possible explanations for an observed abnormal system behavior. % abnormally behaving system. 
%and applies various reasoning techniques to infer from this model and an observed abnormal behavior   is model-based In diagnosis, 
%Truth Maintenance Systems (TMS)~\cite{deKleer1986assumption} and theorem provers are at the heart of classical diagnosis algorithms like the General Diagnosis Engine (GDE)~\cite{de1987diagnosing} and Conflict Directed A* (CDA*)~\cite{williams2007conflict}.\roni{Reading this again, we don't mention here the model at all, only that we reason about something}  


%planning algorithms for classical STRIPS  assume that initial state and the world dynamics (actions preconditiosn and effects) are known and . ~\cite{ghallab2004automated} use a model-based approach which is  manifested by the assumption that the world dynamics are known, and by the well-known ``frame axioms.'' Thinking about it more, the frame axiom 


% Getting models is really hard. But happily enough we got all this data laying around 
As countless applications have discovered, obtaining an accurate model of the world is very difficult. The problems lie in both the {\em scale} and {\em complexity} that such accurate models would require. One of the main hurdles is neatly summarized by the {\em qualification problem}~\cite{mccarthy1987epistemological}: no matter how much we elaborate our formal model, it seems that we can always identify exceptional conditions that the model has failed to capture. Thus, it seems that any models that we can feasibly work with are necessarily only approximations to the true, complex world. Data-driven methods have been proposed as a means to overcome {\em both} of these obstacles~\cite{valiant2000neuroidal,valiant2000robustLogics}. In particular, in addition to naturally solving the problem of large-scale knowledge acquisition, Valiant argues that data-driven mechanisms may also compensate for the kind of errors that such approximate representations introduce in other parts of a system. Such methods assume that the world is observed and these observations are given as input instead of an accurate model of the underlying world. Then, Machine Learning algorithms are used to learn a model that approximates the world, 
%\note{Roni: it is not clear how data-driven methods overcome this problem. Maybe worth to add here something in the form of "... learn a model that {\em appropriately} approximate the world ..." (i.e, add ``appropriately'') to help make the connection that this kind of approximation does this approximation of the world in a good way, as oppose to manually creating an approximate model}  
in a way that allows us to perform model-based reasoning effectively. 
%and planning. 
In fact, some data-driven approaches even skip this part, and directly learn how to act/diagnose/recognize without generating a complete model of the world~\cite{kearns2002POMDPsample}.
%\note{Roni: Brendan, can you fill here the ref. for planning without creating a model? maybe you had in mind some model-free reinforcment learning? if so, I can find a good reference also}
With the growing availability of historical data and computing power, it is reasonable to say that most AI efforts these days are data-driven,
and data-driven algorithms have been proposed for planning~\cite{fern2011first}, diagnosis~\cite{keren2011model,qin2012survey}, and plan recognition~\cite{peng2011helix,tian2016discovering,harpstead2013investigating}. 

% I love this paragraph. Great edits. 


% Data driven is not always great, and model-based is not always great. Here lays the challenge
Clearly, if there is no a-priori knowledge about the world model then data-driven methods are needed. On the other hand, if an accurate, or even an approximate, model of the world is available then it is wasteful to ignore it. %, as it may replace to need for observing much data to reconstruct it. 
Moreover, model-based approaches provide strong theoretical guarantees in terms of completeness and solution quality (e.g., optimality) that can be difficult, if not impossible, to provide without a formal model of the world. % Maybe also talk about a model as being a compact representation of huge amounts of data, and the fact that we do not have inifite data in many case. 
% We aim for the middle ground
% Our goal: when to use which and why
It is the aim of this proposal to study when model-based methods are better than data-driven and vice-versa, and more importantly how to combine them in a principled manner. In particular, we will focus on the {\bf wide spectrum of middle-ground scenarios} between having a {\bf perfect model of the environment without any observations} and having a {\bf large set of observations and no a-prior knowledge} about the world. These kind of model-and-observations scenarios occur frequently in practice and cover a broad range of settings. %, from having both a perfect model and an abundance of observation data to scarce observation data and partial models with limited confidence. 


%where an approximate model of the world is given along with a database of world observations}. 

% The task is challenging - how to combine model and data and outperform current state of the art?
Developing effective algorithms and analyzing them in such model-and-observations settings raises both challenge and opportunities. A key challenge is how to use both sources of information -- (possibly approximate and partial) model and observations? 
We will address this challenge by developing robust, accurate, and effective planning, diagnosis, and plan recognition algorithms that are designed to do so. 
These algorithms will aim to outperform the state-of-the-art of both model-based algorithms and data-driven algorithms. These data-augmented model-based algorithms are concrete contributions of the proposed research to both academics and practitioners. 

% We want to dig deeper, and develop fundamental theoretical understanding of how model can help data-driven methods 
Beyond practical algorithms that exploit model and observation data, we aim for a deeper challenge: what is the theoretical benefit of having an approximate model over a purely data-driven approach? can it allow us to solve harder problems? 
This theoretical study is intended to uncover the properties of a given model that are most helpful, thus also help direct model building efforts. Relatedly, this will also provide insights into when is it useful to learn a model if we are given data alone, and when it is better to directly learn how to act. 
This follows recent work by PI Juba~\cite{juba2013ijcai,juba2015itcs}, building on earlier works by Khardon and Roth~\cite{khardon1997l2r}, suggests that from the computational complexity view point it may be preferable in some cases to avoid building a model altogether and rely only on observed data.  %\note{Roni: Brendan, can you cite some of your papers that are relevant here?}
%Recent work by PI Juba suggest that in some cases building a model is not beneficial~\cite{}. \note{Brendan, I am not sure I am describing correct what you've done. Please verify?} 
%Third, even if such an approximate model is available, it may still be preferable to avoid using it and only rely on the observed data, from the computational complexity view point.


% We will focus on planning, diagnosis, and plan recognition. We are experts in these problems, and are also complementary in our expertiese, so please give us funding!
While these challenges are very  general, we will focus on the specific, yet highly important, tasks of planning, diagnosis, and plan recognition. PI Stern has past experience in developing state-of-the-art planning~\cite{felner2004pha,sharon2013increasing,sharon2015conflict,goldenberg2014enhanced,stern2014potential,maliah2016collaborative}, diagnosis~\cite{stern2012exploring,stern2014model,stern2014hierarchical}, and plan recognition~\cite{mirsky2016sequential} algorithms. 
In parallel, PI Juba's recent work laid the theoretical foundation for learning and using common-sense reasoning for abduction and diagnosis~\cite{juba2016aaai} as well planning~\cite{juba2016jmlr}. %\note{Roni: Brendan, can you put the appropriate citation of your papers in the references in the previous sentence? I think I know the right ones but not 100\% sure.}
%PI Juba's recent work lay the theoretical foundation for our main approaches to meet these challenges~\cite{}, and have exactly studies their application in planning~\cite{} and abductive reasoning~\cite{} (where diagnosis is a special case of). 
%\note{Roni: I'm starting to fill the bib file with my refs. Brendan - TODO-level2: fill your references in the bib file.}
%Notable, most of the PI Stern's work was empirical while most of PI Juba's work has been theoretical, and thus their collaboration in the proposed research is key to its success, utilizing their complementary expertise.  
Thus, the proposed research is especially suited for the complimentary expertise of the PIs. %. PI Stern's work proposed several practical diagnosis~\cite{}, planning~\cite{}, and plan recognition algorithms~\cite{}. 
Moreover, each of the PIs has preliminary work that demonstrate the potential of a model-and-observations approach for planning~\cite{stern2011probably,stern2012exploring,juba2016jmlr} and diagnosis~\cite{elmishali2016dataAugmented}.
\note{Brendan: which of your works can fit here? maybe the NDDS one?}
%% Roni: unfortunately the NDSS paper didn't end up featuring any of the new algorithms. It turns out that the data-oblivious program analysis my co-authors had used for the image filtering application was tight enough that when we simply checked the filter on a test set, it had no false positives and hence we could verify it had PAC validity. The contribution of the paper was really about how to collect the data, which is nontrivial since users' web-surfing is sensitive. I haven't been able to get the co-authors on board with a follow-up study of some more complex data types that might require more interesting algorithms. But I am including the JMLR paper since one key part of that work was to use *both* explicitly given frame axioms together with a learned action model. (The frame axioms were formulated using naf so that they don't require knowledge of the action model to state.) The work could naturally incorporate additional explicit rules.

%\subsection{Background on Automated Planning}
%TODO
%\subsection{Background on Model-based Diagnosis (MBD)}
%TODO
%\subsection{Background on Plan Recognition}
%TODO
%\subsection{Background on PAC Semantic}
%TODO
%\subsection{Background on Learning to Act?}
%TODO
%\subsection{Background on Data-Augemented Reasoning}
%TODO: Talk here about prior work that combined model-based and data-driven approaches.  This will include our works, but also, of course, others. In particular, I think that model-based reenforcement learning works need to be mentioned. 

\subsection{Learning in Classical Planning}
% Related work - learning is not new in planning
%Learning from trajectories how to plan more efficiently is not novel. 
Combining learning and planning is not novel. For example, the Learning as search optimization (LaSO) framework uses learning to identify which states are ``good'' and which are ``bad''
in the context of Beam Search, where good states are those that should be expanded (i.e., stay in the beam) and bad states can be pruned~\cite{xu2007discriminative}. 
Others have studies how to learn from past trajectories how to improve various planning heuristic such as Fast Forward~\cite{yoon2006learning} and Pattern Database~\cite{samadi2008learning}. Jabbari Arfaee et al.~\cite{arfaee2011learning} proposed a bootstrapping techniques for solving a set of problems in a large state space. 
Phillips et al.~\cite{phillips2012graphs} proposed to build an {\em Experience Graph} (E-graph) that is a graph composed of a set of observed trajectories. Then, they proposed a planning algorithm that the search towards directions that are part of the E-graph. 
Similar efforts were done in the context of local search, 
where Griener~\cite{greiner1996palo} proposed to learn
the landscape of the objective function in order to get a probabilistic guarantee over the likelihood that a local optimum have been reached. 
All the above highlights the importance that the planning community gives to  finding novel and effective ways to incorporate knowledge about past data -- e.g., trajectories -- in the classical planning process. In fact, there is even a recently added track in the international planning competition is specifically dedicated to the combination of learning and plannning~\cite{fern2011first}.  




% Since planning and learning work so well together, we need to better understand this relationship
While the above recent success stories of learning for planning is encouraging, a theoretical framework for incorporating prior knowledge is still lacking. We aim to close this gap and provide a {\em theoretical framework for incorporating knowledge from observed trajectories in the process of model-based planning}. Such a framework is sourly needed in order to understand the extent to which trajectories can help to improve planning, as well as to understand how using learning-based heuristic affect the properties of the search algorithms that uses them. In addition, we propose concrete algorithms that will take advantage of this theory to plan more efficiently. 


\subsection{Reinforcement Learning}
\note{Roni: Brendan, can you put here how we are different from standard reinforcement learning? 
I read the JMLR related work section again, but think it will still be better if you put here
how it is different.}


\subsection{Learning in Diagnosis}
\note{TODO For Roni}


Niggemann et al.~\cite{niggemann2012learning} recently proposed a method for learning behavioral models of a system that is described by a hybrid timed automata. They showed that given enough samples they will accurately learn the system, but the number of samples is very large. This learned model was used so far for fault detection~\cite{nig} but not for diagnosis (fault isolation).

Sadov et al.~\cite{sadov2010towards} attempted to learn a partial model and use it to diagnose. Their focus was on how to minimize the number of samples until a useful enough model is obtained. 


\subsection{Diagnosis with an Incomplete Model}
Dexter and Benouarets~\cite{dexter1997model} proposed a model-based algorithm for fault detection and diagnosis that is designed for cases where one has an inaccurate model. Their algorithm identifies faulty components using fuzzy matching, comparing the observations against several fuzzy reference models. 

There is also a line of work on {\em robust fault detection and isolation} (robust FDI)~\cite{chen2012robust,frank1997survey} that is specifically designed to develop fault detection and diagnosis algorithm that are robust to inaccuracies in the system model. Most work on robust FDI do not apply logic-based reasoning and attempt to model the possible ways in which the model can be inaccurate. This is orthogonal to our approach, where we do not assume such a-priori knowledge about the inaccuracy of the model.  



\note{TODO For Roni: say something about diagnosis with Bayesian Networks}

\section{Objectives and Significance}
%2. From BSF guidelines: Objectives and significance of the research

TODO (MORE OR LESS RESTATE WHAT WAS ABOVE IN A CONCISER WAY)

%[[From e-mails]] For our own work, even if we can't nail down precisely when our algorithms work well, we will still strive to obtain some partial understanding of when and why the approach works, as is being done for SAT solvers.


% Broader significance

% Different settings: no model, some model, reliable model
The contributions of the proposed research go far beyond creating better algorithms for diagnosis, planning, and plan recognition.  By studying and understanding the effect of having an approximate model on the theoretical limits of what can be learned from data, 
and what needs to be supplied by the approximate model, one can impact the efforts used to generate such approximate models in a more informed way. For example, if some aspect of the world is especially difficult to learn, then it makes sense to divert expert efforts in modelling it, while if other parts are easier to learn then expert costs can be saved. Since modeling the worlds is notoriously difficult, choosing what not to model is key in practical applications of AI. 
Moreover, we show that one may actually choose to ignore a given model 
if sufficient data is available, in order to speedup the reasoning process. 



%\section{Detailed Description of the Proposed Research}
%3. From BSF guidelines: Comprehensive description of the methodology and plan of operation, including the respective roles of the Israeli and American principal investigators;
\section{Methodology and Plan of Operation}
\label{sec:methodology}


% Key setting: we have a model and observations
The main setting the proposed research will focus on is that we are given two types of data as input: 
a {\em model} and a set of {\em observations}. The model represents some prior knowledge about the rules that govern the behavior of the world. The observations represents data collected about some  interactions with the world in the past. In some sense, the observations represents some knowledge about {\em how the world behaved in the past} and the model also represents knowledge about {\em how the world is expected to behave in the future}. What exactly does these model and observations comprise depends on the reasoning task at hand. In the proposed research we focus on reasoning for two classical AI tasks: planning and diagnosis. 



\subsection{Data-Augmented Planning}

% What is the model in planning
In planning, the task is to create a plan for an agent to follow in order to achieve a designated goal. A model in the planning context is any information the agent has about the current state and how its actions impact the world. For example, in classical planning~\cite{fikes1971strips}, a model can be a description of the pre-conditions and effects of each action given in PDDL (the Planning Domain Description Language). In more involved planning models such as Markov Decision Processes (MDP) and Partially Observable MDPs, the model can also include the transition and observation functions that capture knowledge about the stochatic nature of the world. 


% What are observations in planning
Observations in the context of planning are {\em trajectories} of actions performed by the agent to get from one state to another. These trajectories as sequences of the form $\tuple{ s_1, a_1, s_2, a_2, \ldots}$, where $s_i$ is a state, $a_i$ is the action performed when at state $s_i$, and $s_{i+1}$ is the state the agent has reached after performing action $a_i$ at state $s_i$. In the most minimal sense observing a trajectory simply reveals that it was possible to get from state $s_i$ to state $s_{i+1}$ by applying action $a_i$. % Maybe talk about partial observablity?


% Sometimes we have complete models, so we can plan. But it is still so hard!
In some cases, a complete model of the world may provide a sufficient form of abstraction so that a plans generate for it are useful in practice~\cite{ruml2011line}. Indeed, much work has been devoted throughout the years on solving classical planning problems~\cite{ghallab2004automated,borgo2016planning}. A key challenge in such cases is the complexity of finding a plan, which is PSPACE hard in general~\cite{bylander1994computational} or worse. %cite some impossibility results of POMDP



% We will use observations to help us plan faster!
We propose to study two ways in which observations -- i.e., trajectories of actions performed by the agent in the past -- can help to meet this complexity challenge. The first is to use the given observations to automatically evaluate how different planning algorithms and heuristics perform, and adapt the planning algorithms accordingly. The second is to learn from the given observations special cases of the general planning problem that can be solved efficiently in the domain at hand. We detail both  approaches below. 





% Option #1: PAC Search: what to do with trajectories? how many are useful?
\subsubsection{Probably Approximately Correct Heuristic Search} 
% Approximately optimal
One particular theoretical framework we propose to develop is the Probably Approximately Correct Heuristic Search (PAC Search), which is intended to allow probabilistic solution quality guarantees even for deterministic planning~\cite{stern2011probably,stern2012search}. 
To explain this framework, which was suggested in a preliminary work by PI Stern~\cite{stern2011probably,stern2012search}, we provide the following brief background. 
Heuristic search is a fundamental problem solving technique that is commonly used to solve planning problems. Heuristic search algorithms use heuristics to efficiently explore a usually very large space of possible trajectories, aiming to find a trajectory that forms a sufficient plan. What regards as a sufficient plan depends on the requirements set by the user: in some cases only optimal -- lowest cost -- plans are sufficient while in other cases sub-optimal plans are also acceptable. Since finding optimal plans can be very difficult a common compromise is to require solutions that are {\em approximately optimal}, in the sense that a solution is sufficient if its cost is no larger than $1+\epsilon$ times the cost of the optimal solution, where $\epsilon$ is a parameter set by the user. Such algorithms are called {\em bounded-suboptimal search algorithms}. 


% Approximately optimal sucks
To date, search algorithm that return approximately optimal are severely limited in that they obtain their solution quality guarantee (i.e., at most $1+\epsilon$ of optimal) by consider an {\em admissible} heuristic function. Admissible heuristic function are heuristic function that provide a lower bound on the cost of reaching the goal. By construction, such heuristic function are biased and tend to be inaccurate. Consequently, bounded suboptimal search algorithm often continue the search for better plans even though their incumbent plan (the best plan found so far) is already approximately optimal. In addition, they are limited in their ability to learn from observed trajectories~\cite{ees,egraphs}.

% PAC is so great. It will make world peace. 
The PAC search framework we propose to develop will allow planning algorithms based on heuristic search to fully exploit past experience and observations. In PAC search, which builds on earlier work by Ernandes and Gori~\cite{ernandes2004likely}, where each generated state is associated with an estimate of the likelihood that it will improve on the incumbent solution. This estimate is derived from mining past optimal solutions to similar problems in the same domain. Thus, when a goal is found, one can compute the likelihood that it is optimal or approximately optimal. Beyond the theoretical elegance of having such guarantees, they allow more informed decision making when planning. In particular, one can identify when a sufficient plan has been found earlier than when only using an admissible heuristic, thus significantly speeding up the search. Moreover, probabilistic solution quality guarantees 
provide a more accurate view of the incumbent solution than the current worst-case quality estimate obtained when using admissible heuristics.  


%In that preliminary study the PAC Search framework was only used to provide a better sense of the incumbent solution in an anytime search. 

% But there are so many challenges, you must give us money to study it
While PI Stern's initial study of PAC search showed promising results~\cite{stern2011probably,stern2012search}, there are still many challenges that prevent it from gaining significant adoption and impact. First, current PAC search algorithms require trajectories that are optimal plans. This limits the applicability of PAC search, since finding optimal plans can be very difficult. We will study several ways to meet this challenge. One approach is it to gather statistics on smaller problems which can be solved optimally and extrapolate on larger problems. Another approach is to estimate the suboptimality of observed trajectories, e.g., using solution cost prediction algorithm, such as those developed by PI Stern~\cite{}. 



% Second challenge - sample set
A second challenge to the wide-spread adoption of PAC search, is that there is no clear guideline forhow many trajectories are needed to learn meaningful information about the likelihood of a state to lead to a goal. This is a manifestation of the classical sample set complexity problem that is often studied in the Machine Learning literature. Indeed, PI Juba has vast experience in performing such analysis~\cite{BrendanCanYouPutHereSomeOfYourWork} and is thus the collaboration between the PIs is especially suitable to address these challenges. 


% Option #2: Find tractable classes
\subsubsection{Learning Tractable Classes for Planning}
% Classical planning is hard, but we can solve many problems. How to know which one?
While even classical planning is PSPACE complete~\cite{bylander1994computational}, 
current state-of-the-art planners are able to solve relatively large problems with hundreds of state variables. This success is often attributed to effective heuristic functions and smart planning algorithms that use them. However, it is often the case that one does not know upfront whether a given planning problem will be easy to solve or hard using current planners. It is known that some classes of the planning problem can be solved optimally in polynomial time~\cite{katz2008new}. In addition, the complexity of planning is known to be related to various characteristics of the underlying planning problem, such as the size of the causal graph~\cite{gimenez2012influence}, its relation to the delete relaxation problem~\cite{hoffmann2011analyzing}, and the problem's width~\cite{lipovetzky2012width}. All these works analyze the underlying structure of a planning domain to recognize structures that the respective planning algorithm can solve effectively. %identify if it can be solved easily. %However, a general 

% A key research question - can we learn from experience the tractable problems we can handle, and can we use experience to extend the range of tractable problems
We propose to identify such tractable, or probably tractable, structures in the planning world in an automated way. This is based on the work of PI Juba on learning probably applicable 
actions in a Partially Observable MDP setting~\cite{juba2016jmlr}. He proposed 
an algorithm for learning from observed trajectories sequences of actions that are probably applicable and then, when applicable, use these sequence of actions to achieve planning goals. 
In such cases, planning is almost instantaneously, and the probability of its success is bounded~\cite{juba2016jmlr}. 


% Brief intro to macro actions
Transferring this idea to classical planning allows a principled way to learn {\em macro actions}. A macro action is a sequence of actions the agent can perform that are considered as a single action when planning. Prior work has studies offline and online methods to learn sequence of actions that will be used in planning as macro action to speed up the search~\cite{chrpa2014mum,coles2007marvin,chrpa2015online,koedinger1990abstract,korf1985macro}. A key challenge is how to choose which macro actions to define, since adding macro actions to the set of actions an agent considers when planning can be useful, as it allows reaching deeper states in the search space faster, but it can also be harmful, as adding them increases the branching factor and creates more transpositions. We will develop algorithms for appropriately choosing which sequence of actions should be macros that are based on the frequency of the observed trajectories and the expected usefulness.  


% We can generate all macros and their exact pre and post conditions
There are well-known algorithms to identify all sequences of actions that occur sufficiently frequently in a collection of trajectories~\cite{mannila1997sequences}. The number of such candidate macro actions can be easily bounded by a polynomial in the frequency and the time horizon of the episodes. Because classical planning domains provide complete descriptions of the preconditions and effects of actions, we can easily generate a combined precondition and list of effects for each such candidate macro action.

% The question: how to choose effective macros
The theoretical and practical question that arises is which macro actions to construct that will be most useful to the planner. 
So, considering the observed trajectories, we can choose macro actions that would have been frequently useful in hindsight. 

% The problem: the macro-enabled planner is modified, and who knows where it will end up in
However, the theoretical challenge that now arises is whether or not the macro actions we construct remain useful to the problem in future problems: Observe that even if the trajectories were generated by a fixed planning algorithm on problems drawn from a common distribution, and the planner continues to operate on problems drawn from the same distribution, the availability of new macro actions may cause the planner to include a macro action that is distinct from the action it would have used at a given point in a given plan. Then the trajectories generated by the planner may, for example, include states that would never have been visited by the planner that did not have the macro actions available, and the rest of the macro actions may no longer be useful in this new region of the state space. What we do know is that the macro actions correspond to sequences of actions that occurred frequently in our past trajectories. Intuitively, as long as the states in which the actions were taken were sufficiently ``typical,'' then we expect that the final state after executing the macro should also be relatively ``typical,'' and thus also expect that our estimates of the utility of our new macro actions should only be skewed by a limited amount, depending on just how typical the initial states were and how frequently the macros were used.


\note{Roni: The above is good. Mabye we can put some mathematical formalizm to better convince the reader. I will focus today on the MBD part below and then get back to this.}
%GENERALIZE THE MACRO
%DEFINE STATIONARITY 
%PROBABLY PLANS
%HOW TO CONCATENATE SEQUENCES AND STILL HAVE GUARANTEES
%SOME RAPID MIXING ASSUMPTINO (GETTING TOT HE SAME START STATE DISTRIBUTIONS)


% Option #2: Learn probable macros
%{\bf Learning Macros that will Probably Work.} 
%such macro actions,  which are added to the agent's set of actions when planning. 
%sequence of actions 
%to the set of actions an agent can perform Learning macro actions are known method for learning from trajectories how to improve planning~\cite{korf,others,chrpa2014mum}. 





\subsection{Data-Augmented Model-Based Diagnosis}


% What is diagnosis. It is important
{\em Automated diagnosis} (DX) is the second reasoning task for which we investigate how to exploit both model and observations. The DX problem is to find plausible explanations to an observed abnormally behaving system. Such explanations -- also referred to as {\em diagnoses} -- usually point to one or more components of the observed system that may be faulty. 
Hardware and software systems these days are growing in complexity, and thus effective DX algorithms are crucial. 


% Model-based and data-driven for DX: different inputs
Several approaches have been proposed to solve the DX problem. Two prominent approaches are {\em model-based} and {\em data-driven}. In both approaches some observations of the current behavior are given, denoted by $OBS$. In Model-Based Diagnosis (MBD), we are also given a model of the diagnosed system, which is assumed to describe how  components are expected to behave when functioning properly. In some cases a stronger model is given that also describes the possible behavior of the system components when they are faulty (possibility distinguishing between multiple fault modes). In data-driven diagnosis (DDD), instead of a model of the diagnosed system we are given a large set of such past observations. Importantly, these past observations differ from $OBS$ described above in that they represent observations made in the past. Moreover, each past observation is paired with its root cause -- the set of components that were faulty when that observation was taken. Observe that here too we have the same distinction as in planning -- the data-driven approach has information about how the world behaved in the past while the model-based approach has also knowledge about how the world is expected to behave in the future. 


% Model-based and data-driven for DX: different algorithmic approaches
MBD algorithms usually use inference methods to find diagnoses that will be consistent with both model and $OBS$. In contrast, DDD algorithms employ statistical techniques to learn an efficient function, e.g., a decision tree, that will map future observations to their correct diagnosis. 
MBD has been successfully applied in a range of domains~\cite{williams96,struss2003model,wotawa2002model}, and lays on solid theoretical grounds~\cite{de1987diagnosing,reiter1987theory}. However, MBD has two key limitations. First, it requires a model of the system, which is expensive to create and is often inaccurate. Second, solving a diagnosis problem with MBD is, in terms of computational complexity, intractable~\cite{bylander1991computational}. Computing a diagnosis with DDD, on the other hand, is usually very fast as most effort is done offline during training. However, DDD algorithms lacks, however, formal guarantees to its correctness and performs poorly for observations with multiple faults~\cite{keren2011model}. 



% We will use our complementing expertise to provide both useful algorithms as well as theory for using both sources of information. This will be awesome
The fundamental tenant of our proposed research is that MBD and DDD should complement each other, as they use different types of inputs, and both types of inputs are often available, in some form. Therefore, we propose to develop theory and algorithms that exploit both model -- to the extent that it is available -- and data about past observations, in an effort to enjoy the pros of both approaches. Below we describe the concrete approaches we will follow in this line of research.

% Focus on cases where the model is correct 
First, we investigate how MBD and DDD can be integrated in scenarios where an accurate, even if incomplete models of the diagnosed system is available together with past observations. 


\begin{figure}
    \centering
	%\includegraphics[width=0.6\columnwidth]{mbd-example.pdf}
	\includegraphics[width=0.5\textwidth]{mbd-example_cropped.pdf}
%    \includegraphics[width=0.75\textwidth]{mbd-example}
    \caption{A simple example of a DX problem with multiple consistent diagnoses}
    \label{fig:mbd-example}
\end{figure}

% The problem: too much diagnoses
\subsubsection{Model-Based Reasoning with Data-Driven Prioritization} 
Most MBD algorithms are consistency-based, in the sense that they aim to return every diagnosis  that is consistent with the system model and observations. 
Unfortunately, there can be many consistent diagnoses for a given set of observations, thus providing poor fault isolation~\cite{stern2015many}. 
For example, Figure~\ref{fig:mbd-example} shows a simple DX problem where the diagnosed system is a small Boolean circuit. Components $A$, $D$, $C$, and $E$ are NOT gates, component $B_1$ is an OR gate, and components $B_2$ and $B_3$ are buffer gate (which are supposed to output the same value as their input). One possible diagnosis is that component $B_1$ is faulty, and have outputted zero instead of one. Similarly, either component $B_2$ or $B_3$ may be faulty, 
and there are also possible double fault diagnoses (i.e., diagnoses that assume two components are faulty): $\{ A,D \}$ and $\{C,E\}$. It is difficult to assess which of these five diagnoses is correct or more likely. 
As shown in PI Stern's prior work, even if we only focus on minimal cardinality diagnoses (those with the smallest number of assumed faulty components) the number of possible diagnoses can be very large, even in standard MBD benchmarks~\cite{stern2015many}. Thus, MBD algorithms require some way to prioritize their results diagnoses. 


% The solution: Bayesian reasoning. But that needs priors - DDD to save the day!!!
This is exactly where we propose to employ a data-driven approach -- to prioritize and an informed way between consistent diagnoses. 
DDD algorithms learn from past observations a function that classifies components as healthy or faulty given the current observations and other various model properties. These learned classification model often also provide some confidence measure that estimates the liklihood that the classification is correct. We propose to use this learned function in the diagnostic process to prioritize the consistent diagnoses. So, the resulting algorithm will be a combination of MBD algorithm for generating diagnoses and DDD algorithms to prioritize them. This hybrid model-and-data approach for MBD is very general, and our preliminary results on software diagnosis~\cite{elmishali2016data} show promising results. 

% Future challenges
We intend to formalize this data-augmented MBD approach and demonstrate its usefulness on a range of domains. Also, we will study the relation between the number of past observation we have, the size and structure of the analyzed system, and the expected usefulness of this data-augmented approach over the data-agnostic MBD approach. Since in many cases most past observations are of a healthy scenario and only few faulty scenarios are available, we will study how to build DDD models that can use mostly healthy scenarios and still be able to help in prioritizing diagnoses. An additional opportunity is to guide the MBD algorithm with knowledge from the DDD model. For example, MBD algorithms based on heursitic search, such as HA*~\cite{feldman2006two} and CDA*~\cite{williams2007conflict}, can guide their search towards adding earlier components that are more likely to be part of the diagnoses according to the DDD model. 


%\note{I feel that we need more here. Maybe something about uncovering the theory behind this combination of data and model?}
\subsubsection{Learning Plausible Diagnoses} 
Similar to planning, there are also special case of MBD that can be solved in tractable time. This is acknowledged in the MBD literature, where some types of system models are known to be easy to diagnose~\cite{horn}. Also, several techniques have been proposed to compile a given system model to an easy-to-diagnose model such as Decomposable Negation Normal Form~\cite{darwiche2001decomposable} and Ordered Binary Decision Diagrams (OBDDs)~\cite{torta2006onTheUse}. These compilation methods, however, sometimes results in a model that is exponentially larger than the original system model. 

We propose a general approach that makes use of past observations to identify easy-to-diagnose DX problems. The main idea is to identify classes of DX problems from past observations that one can learn a model to diagnose them. This circumvents the general problem of learning a DDD model, while provides fast and effective solutions to a meaningful subset of the cases. 
PI Juba's recent work~\cite{juba2016aaai} outlined a theory for learning such special cases. However, no empirical support for the usefulness of this theory was given. In the proposed research we will demonstrate its usefulnes on standard diagnosis benchmarks (see the Evaluation section below). Moreover, the model being learnt

\note{Roni: we should add here more challenges, along the lines of the type of models we can learn, the type of special case, the sample complexity of all this, the overall integration with MBD on top of this, etc.}




\subsection{Data-Augmented Reasoning with an Incomplete Model}

% Incomplete models is important
In many cases, having a complete model is not possible, e.g., due to the effort of creating such a model or due to the unexpected nature of the world. A model can be incomplete in many ways. For example, in planning there may be incomplete knowledge about the acting agent's actions, which includes only part of their pre-conditions and effects. Another example is the existence of exogenous events that are not represented in the model. 

% Observations are very important for incomplete models 
Data about observed trajectories can play several roles in such cases of an incomplete model: as a means for learning the incomplete parts of the model, as a way to estimate which parts of the reasoning state space can be reasoned about with reasonable certainty, 
and finally, as a source for speeding up planning as discussed above. 

% This is well-known, but lacking good theory to decide how to reason
While work on reasoning with incomplete models and observations has been discussed ... \note{Roni: we need some strong arguments against what's been done}


We propose to address this setting by using Valiant's PAC semantics~\cite{valiant2000robustLogics}, a particular theoretical framework that is exactly aimed to bridge the gap between having observations to learn from and learning an incomplete and only probably correct model. We will develop concrete algorithms able to perform reasoning tasks -- and in particular planning and diagnosis -- using an underlying PAC semantic knowledge base. 

\note{Roni: something about how PAC semantic are great in balancing knowledge}





\subsubsection{Learning PAC Semantic Rules for Planning}
\note{Roni: TODO: Brendan?}


Problems: 
1. Causality and chaining of rules
2. Which rules are helpful to plan (how to define it, etc.)
3. Trasportability

\subsubsection{Planning with PAC Semantic Rules}


% Having PAC rules is not the end of the problem. No one has used them for planning or diagnosis
After learning a set of rules for planning, there is still the task of actually generating the plan. PAC semantic have been designed explicitly to enable efficient reasoning~\cite{valiant2000robustLogics}. However, they have been applied so far to a limited set of reasoning tasks such as prediction of missing words in text~\cite{michael2008first} and user profiling in a recommender system~\cite{semeraro2009knowledge}. With the the exception of the preliminary work of PI Juba~\cite{juba2016aaai,juba2016jmlr}, PAC semantic rules have not been used to neither planning nor diagnosis, two fundamental AI reasoning tasks. We aim to do so in the proposed research. 


\note{Brendan, does the paragraph below make sense?}
% A concrete explanation of what is a PAC semantice rule in planning
The particular type of PAC semantic rules we aim to reason about in the context of planning are the actions the planning agent can perform. 
That is, we will obtain from learning the set of actions $A$ that the agent is assumed to be able to perform and are useful for planning, 
and an error function that maps every action $a\in A$ 
to the probability that the action's preconditions and effects are true.
Concretely, consider a STRIPS-style action $a$, which is defined by the tuple $\tuple{\pre(a), \eff(a), c(a)}$, corresponding to the preconditions, effects, and cost of $a$, respectively. The corresponding error function value $err(a)$ is the probability that (1) $a$ is applicable when $\pre(a)$ hold, (2) applying $a$ will have cause the effects in $\eff(a)$, and (3) the cost of applying $a$ is $c(a)$. 



% Planning with PAC rules is not trivial
Planning with such PAC semantic rules raises several challenges and opens several possible types of planning problems.
Since every planned action may not have its desired effect (since the learned rules are only probably approxiamtely correct)
then one may wish to generate a plan that will achieve the goal with some amount of certaintly. In addition, it is often desired that the cost of the resulting plan (sum of the costs of its constituent actions) is minimized. A natural some tradeoff between the plan's cost  and its probability of success exists. 

% Our approach: build on existing planners, link to PAC search framework we developed
A natural framework for balancing success probability and solution cost 
is the PAC search framework described above. 
Therefore, proposing intelligent planning algorithm that balances this trandeoff, which arises when using learned PAC semantic rules as actions, is a main objective of this proposed research. 

%Our approach will build on  existing planners and approaches for probabilistic contignet planningand 




\note{Roni: this should probably go into the scientific background}
Of course, planning that considers both uncerainty and plan cost is well-studied. Perhaps the most rich model for planning with uncertainty is
the Partially Observable Markov Decision Problem (POMDP)~\cite{cassandra1994acting}
model, in which agent's actions may have a set of possible outcomes
and the agent may not know its current state with certainty. 
In POMDP, a transition function and an observation function are assumed to be given, which map the probability of reaching a state and obtaining an observation, respectively, given that the agent performed a given action in a given state. Having such an accurate knoweldge about the world's uncertainty is often difficult to obtain and POMDPs are nutoriously difficult to solve and POMDP solvers usually do not scale~\cite{todo}. 
Thus, ``weaker'' models of uncertainty have also been studied. 
For example, {\em conformant planning}~\cite{todo} is the task of generating a plan to reach a goal when the actions have non-deterministic effects
and the initial state is unknown. The desired plan in conformant planning must guarantee that the goal is achieved without any observations collected while executing it. Contingent planning~\cite{} addresses a similar setting, but the task is to generate a conditional plan, deciding in run time which actions to perform by considering collected observations. 
Both conformant and contingent planning have probabilistic versions,
in which the goal is to maximize the probability of success~\cite{blum1999probabilistic,taig2015compilation,markou2016cost} 
or to come up with a plan that achieves the goal with probability that success exceeds the threshold~\cite{kushmerick1995algorithm}.

% Differential 
All the above shows the maturity of the planning literature 
and we intend to build on these success stories when developing our  planning algorithms. However, there are some key differences 
when planning with learned PAC semantic rules. First, we do not expect realistially to be able to learn a full-fledged POMDP model for problems that are not very small. Indeed, learning POMDP models have been studied in the reinforcement learning literature and are known be difficult to scale. 
Second, we deal with two sources of uncertainty -- the actions may not even be applicable and their outcome may be different than planned. In some sense, this is similar to the weak-fault model setting mentioned above in the context of diagnosis -- we do not assume to know what will happen if the action will fail. Perhaps the closest setting is that of probabilistic contingent planning, but planning cost has rarely been addressed in this scenario. 


% This is more than just another planning algorithms - we are ``closing the loop'' of planning and learning
By developing effective planning algorithms able to use learned PAC semantic rules is useful on its own right, as one can imagine other ways in which actions that are only applicable with some probability can arise. However, 
it is important to demonstrate this specifically in the proposed research, to show that learning PAC semantic rules form a viable holistic solution to the problem of planning with an incomplete model. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%Learn conditions on sensors taht yield a small set of possbile diagnoses (see AAAI paper)

\subsubsection{PAC Semantics for Probably Correct Model-based Diagnosis}
\note{Warning: drafts below!}


% In MBD we're used to incomplete models, but the approximate models are harder for us. But still, we rock at reasoning, so what should we do?
Since modeling is known to be very difficult in model-based diagnosis, much of the MBD literature has dealt with weak types of model. One such well-studied weak type of model is the {\em weak fault model} (WFM), in which only the nominal behavior of the components is modeled while there is no knowledge of how components behave when they are faulty. However, even such a weak fault model is usually very difficult to obtain. Indeed, predicting {\em exactly} how components will {\em always} behave in a complex system is very difficult. In particular, obtaining component behavior rules that are always correct is very hard, and learning such rules is practically impossible in most cases. 
On the other hand, efficient MBD algorithms have been developed over the years (some by PI Stern~\cite{metodi2014novel,stern2012exploring,stern2014hierarchical,elmishali2016dataAugmented,lazebnik2016solving}) that apply various forms of reasoning to infer diagnoses based on such models. 


% YES! let's use PAC semantics. Rules are behaviors and their relationship with health
We propose to leverage the power of PAC semantics to enable reasoning with incomplete and inaccurate models. The PAC semantic rules in the context of model-based diagnosis are relations between observed system behavior (e.g., sensor values) and the health of subset of the diagnosed system's components. 
A simple example is Horn-clause type of rules, e.g., $h(A)\rightarrow (o\equiv i_1 \wedge i2)$
where $A$ is a component in the system, $h(A)$ is a predicate denoting that $A$ is healthy, and $i_1$, $i_2$, $o$ are the input and output values of $A$, respectively. 
Such rules are commonly used in MBD algorithms, but the novelty is the error function that is added when using PAC semantics, associating each rule with the probability that it is false. 


% Challenges in learning such rules
Learning PAC semantic rules for MBD raises similar challenges to those outlined above for learning PAC semantic rules for planning. 
One challenge is how to choose which rules to learn. In MBD, the tradeoff is between  diagnostic accuracy and complexity: more rules may allow finding more accurate diagnoses but at the cost of greater complexity. 
A second challenge is ... \note{Roni: TODO for me: put here something about chaining and transportability after reading that part for planning}




% Learning exactly what each component is hard, but maybe learning broader rules will be easier.
To simplify the learning task and to be able to obtain more information from past observations, we intend to learn and reason about more complex rules than simple Horn clauses. First, we will consider rules where the left hand side consists more than a single component,  e.g., $(h(A)\wedge h(B)) \rightarrow o\equiv i_1\wedge i_2$, where $B$ is another component. Such rules are expected to be easier to learn from data, as they may require less detailed knowledge about the internals of the diagnosed system. Moreover, rules in which observed values exonerate some components from the suspicious of being fault are also possible~\cite{struss1989physical}. 
Finally, rules that include both health predicates and sensor values in their left-hand side are also possible, e.g., 
$(in_1=1 \wedge h(A) \wedge h(B))\rightarrow o\equiv 1$. Existing MBD algorithms are capable of reasoning with all these types of rules to obtain consistent diagnoses. 


% Statement of operation: we will reason about rules that are only sometimes true
However, most MBD algorithm do not reason about the likelihood that the rules they use for reasoning -- i.e., the system model -- may be incorrect. A key objective in the proposed research is to study exactly this: develop MBD algorithm that are able to reason with PAC 
semantic rules. 

% The problem 
Concretely, the diagnostic challenge to find the most likely diagnosis (or a set of highly likely diagnoses) given a set of rules that describe various relations between observed system behavior and components' health, such that each rule is associated with a probability that it is true, referred to as the rule's {\em validity}. 

% Why it is not trivial
A simple approach can be to limit the diagnosis algorithm to only use rules whose validity is over some given value. This approach has several drawbacks. First, there is no principled way to set this threshold. Second, this approach can result in suboptimal results, as follows. 
Assume that we have 3 rules $r_1$, $r_2$, and $r_3$ such that the probabilities that each rule is correct are 0.8, 0.8, and 0.7, respectively. 
Now, imagine that there are two diagnoses $\omega_1$ and $\omega_2$, such that $\omega_1$ is derived by reasoning about $r_1$ and $r_2$, while $\omega_2$ is derived by reasoning only about $r_3$. Which diagnosis is more likely? 
if we assume that rules likelihoods are independent, than clearly $\omega_2$ is more likely. But, if we set the rules threshold to 0.75, our diagnosis algorithm will not find $\omega_2$. 

% We will solve it!
Thus, when diagnosing with PAC semantic rules,  the likelihood of a diagnosis is affected by  the validity of the rules used to infer it as well as any other prior knowledge about the components failure likelihood (such as the learned priors discussed in earlier in this proposal). We will develop diagnosis algorithms based on heuristic search that search for the most likely diagnoses in this setting. Developing these algorithms as well as appropriate heuristic is part of the proposed research. 


% Diagnosis for physical systems
Above, we have implicitly assumed that logical rules can be learned to approximately represent the  diagnosed. However, systems may be too complex to be represented in logical, qualitative means. In such cases, the learned rules will include learning functions that approximate some parts of the system behavior. While learning a complete model of a physical system is bound to be futile, we will aim for learning cases that are common enough and can be approximated by a learned function. A preliminary framework for how to do so was recently outlined by PI Juba~\cite{juba2016aaai,juba2016conditional}.




% Wrapping it up -- a complete learning and diagnosing framework that exploits the benefits of PAC semantics and lay on sound theoretical ground
As in planning, the merit of developing diagnosis algorithm able to reason with PAC semantic rules is especially important to demonstrate the ability to perform a complex reasoning task like diagnosis without a complete system model and based on the theoretical grounds of PAC semantics. 











\subsection{Evaluation}

We will take standard benchmarks for each of these problems and will add ``noise'' to the model. Then, compare model-based methods that only reason about the noisy model, data-driven methods that attempt to directly answer the quetion (not sure this fits for planning, but it does for diagnosis and plan recognition), data-driven methods that learn an model and then apply reasoning, 
and then some new methods by us that integrate all these things nicely. 


%4. From BSF guidelines: Risk analysis and alternative paths that will be followed if the suggested research plan fails (only in those fields in which it is relevant);
%\section{Risk Analysis and Alternative Plans}
%\note{Maybe we want to avoid this section, even though it is in the guidelines}

\section{Collaboration between the Principal Investigators}
PI Juba and PI Stern met when they were both post-doctoral fellows as the School of Engineering and Applied Sciences (SEAS) in Harvard University. 
While PI Juba and PI Stern study very different aspects of the broad Artificial Intelligence fields, they have had during that time many scientific discussions on some of the topics discussed in this proposal. However, PI Stern soon returned to Israel for a faculty position at BGU, before these discussions could mature into a fruitful scientific collaboration. 
Throughout the years, the PIs have met in general AI conferences
and recently their research interests have aligned, 
with the recent work of PI Juba on the theoretical foundation of learning for planning and abductive reasoning, and the years of experience in developing planning and diagnosis (a form of abductive reasoning) algorithms. 
Both PIs are young researchers and hope to develop this joint interest to a long-term collaboration on the fundamental topics outlined in this proposal, but funding is needed to support it.  

%5. From BSF guidelines: An account of available U.S. and Israeli resources, including all personnel and equipment relevant to the research;
\section{Available U.S. and Israeli Resources}
One Ph.D student and one Masters students from PI Sterns group, which currently includes 3 Ph.D. and 10 Master's students, will participate in the proposed research. 
\note{Roni: Brendan, please fill in your estimate of how many people will work on the project. From our discussion on budget I think you aim for one student, right?}
The facilities available at BGU for PI Sterns
lab will be available for this research, including basic office facilities and a server for performing heavy duty computations. BGU and WUSTL will provide the facilities for holding the annual research meetings.

%[Avi, Michal, Iliya, Barak, Amir, Orel, Hila, Ester, Netanel, Gal, Dor, Daniel, Yossi, ]



\pagebreak
\bibliographystyle{plain}
\bibliography{references}

\end{document}

